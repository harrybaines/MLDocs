{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Machine Learning Algorithms The algorithms and explanations listed here are intended to give a quick overview of how each algorithm works along with code examples to show how you can implement them. Library versions are provided along with the from-scratch implementations. Links to further resources are also given. The following algorithms are given: Linear Regression Linear Regression Overview \\[ y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n \\] To train the regression model, we can use the Mean Square Error performance metric to find the parameters which minimise the error: \\[ \\mathrm{MSE}(\\boldsymbol{\\theta}) = \\frac{1}{m}\\sum^m_{i=1}(\\boldsymbol{\\theta}^{\\mathrm{T}} \\mathbf{x}^{(i)} - y^{(i)})^2 \\] We can use the Normal Equation (closed-form solution) to find \\(\\boldsymbol{\\theta}\\) (i.e. the parameters which minimise the MSE): \\[ \\boldsymbol{\\hat{\\theta}} = (\\boldsymbol{\\mathrm{X}}^{\\mathrm{T}} \\boldsymbol{\\mathrm{X}})^{-1} \\boldsymbol{\\mathrm{X}}^{\\mathrm{T}} \\boldsymbol{\\mathrm{y}} \\] Implementation import pandas as pd import numpy as np def concat_ones ( X ): # Adds a column of 1's to the matrix X n_items = X . shape [ 0 ] X_b = np . c_ [ np . ones (( n_items , 1 )), X ] # add x0 = 1 to each instance return X_b def lr_normal_train ( X , y ): # Uses the Normal Equation to compute parameters for Linear Regression n_items = X . shape [ 0 ] X_b = concat_ones ( X ) theta_best = np . linalg . inv ( X_b . T . dot ( X_b )) . dot ( X_b . T ) . dot ( y ) return theta_best Normal Equation Complexity In practice, it is best to avoid using the Normal Equation due to the computational complexity ( \\(\\mathrm{O}(\\mathrm{n}^3)\\) ) of inverting the matrix \\(\\boldsymbol{\\mathrm{X}}\\) . This means doubling the number of features increases computation time by \\(2^3 = 8\\) times.","title":"Machine Learning Algorithms"},{"location":"#machine-learning-algorithms","text":"The algorithms and explanations listed here are intended to give a quick overview of how each algorithm works along with code examples to show how you can implement them. Library versions are provided along with the from-scratch implementations. Links to further resources are also given. The following algorithms are given: Linear Regression","title":"Machine Learning Algorithms"},{"location":"#linear-regression","text":"","title":"Linear Regression"},{"location":"#overview","text":"\\[ y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n \\] To train the regression model, we can use the Mean Square Error performance metric to find the parameters which minimise the error: \\[ \\mathrm{MSE}(\\boldsymbol{\\theta}) = \\frac{1}{m}\\sum^m_{i=1}(\\boldsymbol{\\theta}^{\\mathrm{T}} \\mathbf{x}^{(i)} - y^{(i)})^2 \\] We can use the Normal Equation (closed-form solution) to find \\(\\boldsymbol{\\theta}\\) (i.e. the parameters which minimise the MSE): \\[ \\boldsymbol{\\hat{\\theta}} = (\\boldsymbol{\\mathrm{X}}^{\\mathrm{T}} \\boldsymbol{\\mathrm{X}})^{-1} \\boldsymbol{\\mathrm{X}}^{\\mathrm{T}} \\boldsymbol{\\mathrm{y}} \\]","title":"Overview"},{"location":"#implementation","text":"import pandas as pd import numpy as np def concat_ones ( X ): # Adds a column of 1's to the matrix X n_items = X . shape [ 0 ] X_b = np . c_ [ np . ones (( n_items , 1 )), X ] # add x0 = 1 to each instance return X_b def lr_normal_train ( X , y ): # Uses the Normal Equation to compute parameters for Linear Regression n_items = X . shape [ 0 ] X_b = concat_ones ( X ) theta_best = np . linalg . inv ( X_b . T . dot ( X_b )) . dot ( X_b . T ) . dot ( y ) return theta_best Normal Equation Complexity In practice, it is best to avoid using the Normal Equation due to the computational complexity ( \\(\\mathrm{O}(\\mathrm{n}^3)\\) ) of inverting the matrix \\(\\boldsymbol{\\mathrm{X}}\\) . This means doubling the number of features increases computation time by \\(2^3 = 8\\) times.","title":"Implementation"},{"location":"concepts/overfitting-underfitting/","text":"Overfitting/Underfitting What is overfitting? Overfitting occurs when there is a large gap between the training error and the test error. In other words, it models the training data too well. It learns the noise in the training data, which negatively impacts the models generalisation ability.","title":"Overfitting/Underfitting"},{"location":"concepts/overfitting-underfitting/#overfittingunderfitting","text":"","title":"Overfitting/Underfitting"},{"location":"concepts/overfitting-underfitting/#what-is-overfitting","text":"Overfitting occurs when there is a large gap between the training error and the test error. In other words, it models the training data too well. It learns the noise in the training data, which negatively impacts the models generalisation ability.","title":"What is overfitting?"},{"location":"reinforcement-learning/","text":"Bellman Equation Bellman Optimality Equation for state-action value function: \\[ q_*(s, a) = \\mathcal{R}^{a}_{s} + \\gamma \\sum_{s' \\in S}\\mathcal{P}^{a}_{ss'}\\max_{a'}q_*(s',a') \\]","title":"Bellman Equation"},{"location":"reinforcement-learning/#bellman-equation","text":"Bellman Optimality Equation for state-action value function: \\[ q_*(s, a) = \\mathcal{R}^{a}_{s} + \\gamma \\sum_{s' \\in S}\\mathcal{P}^{a}_{ss'}\\max_{a'}q_*(s',a') \\]","title":"Bellman Equation"}]}