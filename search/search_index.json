{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MLDocs MLDocs is a convenient, searchable, waffle-free, cheatsheet-like website which covers many foundational and more technically in-depth machine learning code and equations. In going through various online courses, books and technical docs, I quickly discovered the need for a resource to provide a handy reference to refer back to as and when needed. Machine learning is a complex subject with many moving parts - it can be intimidating and easy to forget things. Any algorithms and explanations given are intended to be as concise as possible. Where relevant, links to code examples in Google Colab will be provided. This is an ever-growing site and is actively maintained!","title":"Home"},{"location":"#mldocs","text":"MLDocs is a convenient, searchable, waffle-free, cheatsheet-like website which covers many foundational and more technically in-depth machine learning code and equations. In going through various online courses, books and technical docs, I quickly discovered the need for a resource to provide a handy reference to refer back to as and when needed. Machine learning is a complex subject with many moving parts - it can be intimidating and easy to forget things. Any algorithms and explanations given are intended to be as concise as possible. Where relevant, links to code examples in Google Colab will be provided. This is an ever-growing site and is actively maintained!","title":"MLDocs"},{"location":"algorithms/gradient-descent/","text":"","title":"Gradient Descent"},{"location":"algorithms/linear-regression/","text":"Linear Regression \\[ y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n \\] To train the regression model, we can use the Mean Square Error performance metric to find the parameters which minimise the error: \\[ \\mathrm{MSE}(\\boldsymbol{\\theta}) = \\frac{1}{m}\\sum^m_{i=1}(\\boldsymbol{\\theta}^{\\mathrm{T}} \\mathbf{x}^{(i)} - y^{(i)})^2 \\] We can use the Normal Equation (closed-form solution) to find \\(\\boldsymbol{\\theta}\\) (i.e. the parameters which minimise the MSE): \\[ \\boldsymbol{\\hat{\\theta}} = (\\boldsymbol{\\mathrm{X}}^{\\mathrm{T}} \\boldsymbol{\\mathrm{X}})^{-1} \\boldsymbol{\\mathrm{X}}^{\\mathrm{T}} \\boldsymbol{\\mathrm{y}} \\] Implementation import pandas as pd import numpy as np def concat_ones ( X ): # Adds a column of 1's to the matrix X n_items = X . shape [ 0 ] X_b = np . c_ [ np . ones (( n_items , 1 )), X ] # add x0 = 1 to each instance return X_b def lr_normal_train ( X , y ): # Uses the Normal Equation to compute parameters for Linear Regression n_items = X . shape [ 0 ] X_b = concat_ones ( X ) theta_best = np . linalg . inv ( X_b . T . dot ( X_b )) . dot ( X_b . T ) . dot ( y ) return theta_best Normal Equation Complexity In practice, it is best to avoid using the Normal Equation due to the computational complexity ( \\(\\mathrm{O}(\\mathrm{n}^3)\\) ) of inverting the matrix \\(\\boldsymbol{\\mathrm{X}}\\) . This means doubling the number of features increases computation time by \\(2^3 = 8\\) times.","title":"Linear Regression"},{"location":"algorithms/linear-regression/#linear-regression","text":"\\[ y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n \\] To train the regression model, we can use the Mean Square Error performance metric to find the parameters which minimise the error: \\[ \\mathrm{MSE}(\\boldsymbol{\\theta}) = \\frac{1}{m}\\sum^m_{i=1}(\\boldsymbol{\\theta}^{\\mathrm{T}} \\mathbf{x}^{(i)} - y^{(i)})^2 \\] We can use the Normal Equation (closed-form solution) to find \\(\\boldsymbol{\\theta}\\) (i.e. the parameters which minimise the MSE): \\[ \\boldsymbol{\\hat{\\theta}} = (\\boldsymbol{\\mathrm{X}}^{\\mathrm{T}} \\boldsymbol{\\mathrm{X}})^{-1} \\boldsymbol{\\mathrm{X}}^{\\mathrm{T}} \\boldsymbol{\\mathrm{y}} \\]","title":"Linear Regression"},{"location":"algorithms/linear-regression/#implementation","text":"import pandas as pd import numpy as np def concat_ones ( X ): # Adds a column of 1's to the matrix X n_items = X . shape [ 0 ] X_b = np . c_ [ np . ones (( n_items , 1 )), X ] # add x0 = 1 to each instance return X_b def lr_normal_train ( X , y ): # Uses the Normal Equation to compute parameters for Linear Regression n_items = X . shape [ 0 ] X_b = concat_ones ( X ) theta_best = np . linalg . inv ( X_b . T . dot ( X_b )) . dot ( X_b . T ) . dot ( y ) return theta_best Normal Equation Complexity In practice, it is best to avoid using the Normal Equation due to the computational complexity ( \\(\\mathrm{O}(\\mathrm{n}^3)\\) ) of inverting the matrix \\(\\boldsymbol{\\mathrm{X}}\\) . This means doubling the number of features increases computation time by \\(2^3 = 8\\) times.","title":"Implementation"},{"location":"concepts/dls-course-2/","text":"Train/Dev/Test Sets Bias/Variance Regularisation \\(L_2\\) Regularisation (weight decay) Dropout (inverted dropout) Norms Frobenius Norm Normalisation Standardisation Vanishing/Exploding Gradients Weight Initialisation for Deep Networks Zero Initialisation Random Initialisation Xavier Initialisation He Initialisation Gradient Checking Gradient Descent Batch Gradient Descent Mini-batch Gradient Descent Stochastic Gradient Descent Exponentially Weight Moving Averages Bias Correction Gradient Descent with Momentum RMSprop Adam Optimisation Algorithm Learning Rate Decay Exponential Decay Discrete Staircase Manual Decay Scheduling Hyperparameter Tuning Batch Normalisation","title":"Deep Learning Specialisation"},{"location":"concepts/dls-course-2/#traindevtest-sets","text":"","title":"Train/Dev/Test Sets"},{"location":"concepts/dls-course-2/#biasvariance","text":"","title":"Bias/Variance"},{"location":"concepts/dls-course-2/#regularisation","text":"","title":"Regularisation"},{"location":"concepts/dls-course-2/#l_2-regularisation-weight-decay","text":"","title":"\\(L_2\\) Regularisation (weight decay)"},{"location":"concepts/dls-course-2/#dropout-inverted-dropout","text":"","title":"Dropout (inverted dropout)"},{"location":"concepts/dls-course-2/#norms","text":"","title":"Norms"},{"location":"concepts/dls-course-2/#frobenius-norm","text":"","title":"Frobenius Norm"},{"location":"concepts/dls-course-2/#normalisation","text":"","title":"Normalisation"},{"location":"concepts/dls-course-2/#standardisation","text":"","title":"Standardisation"},{"location":"concepts/dls-course-2/#vanishingexploding-gradients","text":"","title":"Vanishing/Exploding Gradients"},{"location":"concepts/dls-course-2/#weight-initialisation-for-deep-networks","text":"","title":"Weight Initialisation for Deep Networks"},{"location":"concepts/dls-course-2/#zero-initialisation","text":"","title":"Zero Initialisation"},{"location":"concepts/dls-course-2/#random-initialisation","text":"","title":"Random Initialisation"},{"location":"concepts/dls-course-2/#xavier-initialisation","text":"","title":"Xavier Initialisation"},{"location":"concepts/dls-course-2/#he-initialisation","text":"","title":"He Initialisation"},{"location":"concepts/dls-course-2/#gradient-checking","text":"","title":"Gradient Checking"},{"location":"concepts/dls-course-2/#gradient-descent","text":"","title":"Gradient Descent"},{"location":"concepts/dls-course-2/#batch-gradient-descent","text":"","title":"Batch Gradient Descent"},{"location":"concepts/dls-course-2/#mini-batch-gradient-descent","text":"","title":"Mini-batch Gradient Descent"},{"location":"concepts/dls-course-2/#stochastic-gradient-descent","text":"","title":"Stochastic Gradient Descent"},{"location":"concepts/dls-course-2/#exponentially-weight-moving-averages","text":"","title":"Exponentially Weight Moving Averages"},{"location":"concepts/dls-course-2/#bias-correction","text":"","title":"Bias Correction"},{"location":"concepts/dls-course-2/#gradient-descent-with-momentum","text":"","title":"Gradient Descent with Momentum"},{"location":"concepts/dls-course-2/#rmsprop","text":"","title":"RMSprop"},{"location":"concepts/dls-course-2/#adam-optimisation-algorithm","text":"","title":"Adam Optimisation Algorithm"},{"location":"concepts/dls-course-2/#learning-rate-decay","text":"","title":"Learning Rate Decay"},{"location":"concepts/dls-course-2/#exponential-decay","text":"","title":"Exponential Decay"},{"location":"concepts/dls-course-2/#discrete-staircase","text":"","title":"Discrete Staircase"},{"location":"concepts/dls-course-2/#manual-decay","text":"","title":"Manual Decay"},{"location":"concepts/dls-course-2/#scheduling","text":"","title":"Scheduling"},{"location":"concepts/dls-course-2/#hyperparameter-tuning","text":"","title":"Hyperparameter Tuning"},{"location":"concepts/dls-course-2/#batch-normalisation","text":"","title":"Batch Normalisation"},{"location":"concepts/overfitting-underfitting/","text":"Overfitting/Underfitting What is overfitting? Overfitting occurs when there is a large gap between the training error and the test error. In other words, it models the training data too well. It learns the noise in the training data, which negatively impacts the models generalisation ability.","title":"Overfitting/Underfitting"},{"location":"concepts/overfitting-underfitting/#overfittingunderfitting","text":"","title":"Overfitting/Underfitting"},{"location":"concepts/overfitting-underfitting/#what-is-overfitting","text":"Overfitting occurs when there is a large gap between the training error and the test error. In other words, it models the training data too well. It learns the noise in the training data, which negatively impacts the models generalisation ability.","title":"What is overfitting?"},{"location":"equations/norms/","text":"Norms \\(L_p\\) Norm Computes the size/length/magnitude of a vector \\[ \\ell_p = \\left(\\sum^N_{i=1} |x_i|^p \\right)^{1/p}, \\text{for } p \\geq 1 \\] Implementation NumPy import numpy as np a = np . array ([ 1 , 2 , 3 , 4 , 5 ]) // L1 Norm l1_norm = np . linalg . norm ( a , ord = 1 ) // L2 Norm ( Euclidean norm ) l2_norm = np . linalg . norm ( a , ord = 2 ) // Squared L2 Norm l2_norm_sq = np . linalg . norm ( a , ord = 2 ) ** 2 l2_norm_sq = a . T . dot ( a ) // L \u221e Norm ( Max Norm ) linf_norm = np . linalg . norm ( a , ord = np . inf ) linf_norm = np . max ( np . abs ( a )) From Scratch a = [ 1 , 2 , 3 , 4 , 5 ] // L1 Norm l1_norm = sum ( abs ( num ) for num in a ) // L2 Norm ( Euclidean norm ) l2_norm = sum ([ num ** 2 for num in a ]) ** 0.5 // Squared L2 Norm l2_norm_sq = sum ([ num ** 2 for num in a ]) // L \u221e Norm ( Max Norm ) linf_norm = max ( abs ( num ) for num in a ) NumPy reference for norms","title":"Norms"},{"location":"equations/norms/#norms","text":"","title":"Norms"},{"location":"equations/norms/#l_p-norm","text":"Computes the size/length/magnitude of a vector \\[ \\ell_p = \\left(\\sum^N_{i=1} |x_i|^p \\right)^{1/p}, \\text{for } p \\geq 1 \\] Implementation NumPy import numpy as np a = np . array ([ 1 , 2 , 3 , 4 , 5 ]) // L1 Norm l1_norm = np . linalg . norm ( a , ord = 1 ) // L2 Norm ( Euclidean norm ) l2_norm = np . linalg . norm ( a , ord = 2 ) // Squared L2 Norm l2_norm_sq = np . linalg . norm ( a , ord = 2 ) ** 2 l2_norm_sq = a . T . dot ( a ) // L \u221e Norm ( Max Norm ) linf_norm = np . linalg . norm ( a , ord = np . inf ) linf_norm = np . max ( np . abs ( a )) From Scratch a = [ 1 , 2 , 3 , 4 , 5 ] // L1 Norm l1_norm = sum ( abs ( num ) for num in a ) // L2 Norm ( Euclidean norm ) l2_norm = sum ([ num ** 2 for num in a ]) ** 0.5 // Squared L2 Norm l2_norm_sq = sum ([ num ** 2 for num in a ]) // L \u221e Norm ( Max Norm ) linf_norm = max ( abs ( num ) for num in a ) NumPy reference for norms","title":"\\(L_p\\) Norm"},{"location":"reinforcement-learning/","text":"Key Ideas Reward Function Finite-horizon undiscounted return: \\[ R(\\tau)=\\sum_{t=0}^{T} r_{t} \\] Infinite-horizon discounted return: \\[ R(\\tau)=\\sum_{t=0}^{\\infty} \\gamma^{t} r_{t},\\quad \\gamma \\in(0,1) \\] Value Functions On-Policy Value Function: \\[ V^{\\pi}(s)=\\underset{\\tau \\sim \\pi}{\\mathbb{E}}\\left[R(\\tau) \\mid s_{0}=s\\right] \\] Bellman Equation Bellman Optimality Equation for state-action value function: \\[ q_*(s, a) = \\mathcal{R}^{a}_{s} + \\gamma \\sum_{s' \\in S}\\mathcal{P}^{a}_{ss'}\\max_{a'}q_*(s',a') \\] References OpenAI Spinning Up Documentation Part 1: Key Concepts in RL","title":"Key Ideas"},{"location":"reinforcement-learning/#key-ideas","text":"","title":"Key Ideas"},{"location":"reinforcement-learning/#reward-function","text":"Finite-horizon undiscounted return: \\[ R(\\tau)=\\sum_{t=0}^{T} r_{t} \\] Infinite-horizon discounted return: \\[ R(\\tau)=\\sum_{t=0}^{\\infty} \\gamma^{t} r_{t},\\quad \\gamma \\in(0,1) \\]","title":"Reward Function"},{"location":"reinforcement-learning/#value-functions","text":"On-Policy Value Function: \\[ V^{\\pi}(s)=\\underset{\\tau \\sim \\pi}{\\mathbb{E}}\\left[R(\\tau) \\mid s_{0}=s\\right] \\]","title":"Value Functions"},{"location":"reinforcement-learning/#bellman-equation","text":"Bellman Optimality Equation for state-action value function: \\[ q_*(s, a) = \\mathcal{R}^{a}_{s} + \\gamma \\sum_{s' \\in S}\\mathcal{P}^{a}_{ss'}\\max_{a'}q_*(s',a') \\]","title":"Bellman Equation"},{"location":"reinforcement-learning/#references","text":"OpenAI Spinning Up Documentation Part 1: Key Concepts in RL","title":"References"}]}