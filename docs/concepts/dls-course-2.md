<!-- Week 1 -->
# Train/Dev/Test Sets

# Bias/Variance

# Regularisation

## $L_2$ Regularisation (weight decay)

## Dropout (inverted dropout)

# Norms

## Frobenius Norm

# Normalisation

# Standardisation

# Vanishing/Exploding Gradients

# Weight Initialisation for Deep Networks

## Zero Initialisation

## Random Initialisation

## Xavier Initialisation

## He Initialisation

# Gradient Checking

<!-- Week 2 -->
# Gradient Descent

## Batch Gradient Descent

## Mini-batch Gradient Descent

## Stochastic Gradient Descent

# Exponentially Weight Moving Averages

# Bias Correction

# Gradient Descent with Momentum

# RMSprop

# Adam Optimisation Algorithm

# Learning Rate Decay

## Exponential Decay

## Discrete Staircase

## Manual Decay

## Scheduling

<!-- Week 3 -->
# Hyperparameter Tuning

# Batch Normalisation